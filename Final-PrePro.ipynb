{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "fbfc6f2d-bb26-4e10-bd2a-09581f0ad98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'bciexp', 'subject'])"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trial Based Data Pre-Processing\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import mne \n",
    "mne.set_log_level('error')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "TrialData = loadmat('P08.mat')\n",
    "type(TrialData)\n",
    "TrialData.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "9c15493d-c9ac-4bb4-8c57-d6a1174bcea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thake\\AppData\\Local\\Temp\\ipykernel_20000\\1316363568.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  sfreq = float(BCI['srate'][0, 0])  # Sampling frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 12, 4000)\n"
     ]
    }
   ],
   "source": [
    "#filtering with MNE lib\n",
    "\n",
    "BCI = TrialData['bciexp']\n",
    "EEGData = BCI['data'][0,0] #nChannles x nSamples x nTrials\n",
    "#print(BCI['label'])\n",
    "sfreq = float(BCI['srate'][0, 0])  # Sampling frequency\n",
    "n_channels = EEGData.shape[0]\n",
    "#ch_names = BCI['label'][0,0] # Dummy channel names, replace as needed\n",
    "\n",
    "ch_names = ['P7', 'P5', 'P3', 'P4','P6', 'P8', 'PO7', 'PO3', 'PO4', 'PO8', 'O1', 'O2']\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "\n",
    "# Reshape the data to (nTrials, nChannels, nSamples) as required by MNE\n",
    "EEGData_mne = np.transpose(EEGData, (2, 0, 1))\n",
    "\n",
    "# Create MNE EpochsArray object\n",
    "epochs = mne.EpochsArray(EEGData_mne, info)\n",
    "\n",
    "# Now apply the filter using MNE's filter function\n",
    "filtered_epochs = epochs.filter(l_freq=1, h_freq=15, fir_design='firwin', filter_length='auto')\n",
    "#filtered_epochs = filtered_epochs.resample(64)\n",
    "# To visualize or further process filtered data\n",
    "filtered_data = filtered_epochs.get_data()\n",
    "print(filtered_data.shape)  # This will still be (nTrials, nChannels, nSamples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "70200a56-693b-4d5c-b81e-6cd0260a6551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "stim_id = BCI['stim'][0,0]\n",
    "intention = BCI['intention'][0,0]\n",
    "Expec = BCI['expected'][0,0]\n",
    "Target = BCI['targetside'][0,0]\n",
    "\n",
    "# Assuming `EEG_data` is your filtered and resampled EEG data\n",
    "# stim, intention, targetside are your other fields\n",
    "n_trials = filtered_data.shape[0]\n",
    "print(Target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "169b083f-e97f-49e7-b10f-b208728f3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 12, 250)\n",
      "Condition labels: [1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "#Re-epoching of data based on stim_ch 1 & 2\n",
    "\n",
    "# Parameters for re-epoching\n",
    "stim_id = BCI['stim'][0,0]\n",
    "tmin, tmax = -0.2, 0.8  # Time window around the stimulus (200 ms before, 800 ms after)\n",
    "sfreq = 250  # Sampling frequency\n",
    "n_trials = filtered_data.shape[0]  # Total number of trials\n",
    "condition_labels = []  # To store labels for each re-epoch based on the four conditions\n",
    "\n",
    "# Create an empty list to hold re-epoch data\n",
    "re_epoch_data = []\n",
    "\n",
    "# Loop through each trial\n",
    "for trial_id in range(n_trials):\n",
    "    EEG_trial = filtered_data[trial_id, :, :]  # Extract the EEG data for this trial\n",
    "\n",
    "    # Get the stimulus onsets and target side for this trial\n",
    "    stim_ch1 = stim_id[0, :, trial_id]  # Stimulus channel 1\n",
    "    stim_ch2 = stim_id[1, :, trial_id]  # Stimulus channel 2\n",
    "    targetside_trial = Target[0, :, trial_id]  # Target side for this trial\n",
    "\n",
    "    # Find the indices (sample points) where stimulus onset occurs\n",
    "    stim_onsets_ch1 = np.where(stim_ch1 == 1)[0]  # Stimulus onset for Ch1\n",
    "    stim_onsets_ch2 = np.where(stim_ch2 == 1)[0]  # Stimulus onset for Ch2\n",
    "\n",
    "    # Combine stimulus onsets and create a list of (onset, stim_type) for labeling\n",
    "    all_stimuli_onsets = [(onset, 1) for onset in stim_onsets_ch1] + \\\n",
    "                         [(onset, 2) for onset in stim_onsets_ch2]\n",
    "\n",
    "    # Sort all stimuli onsets by time (ascending order)\n",
    "    all_stimuli_onsets.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Loop through each stimulus onset to re-epoch the data\n",
    "    for onset, stim_type in all_stimuli_onsets:\n",
    "        # Convert onset to seconds\n",
    "        onset_sec = onset / sfreq\n",
    "\n",
    "        # Find the sample indices for the time window around the stimulus onset\n",
    "        start_sample = int(onset + tmin * sfreq)\n",
    "        end_sample = int(onset + tmax * sfreq)\n",
    "\n",
    "        # Ensure indices are within bounds\n",
    "        if start_sample >= 0 and end_sample < EEG_trial.shape[1]:\n",
    "            # Extract the EEG segment for this stimulus onset\n",
    "            re_epoch = EEG_trial[:, start_sample:end_sample]\n",
    "\n",
    "            # Append to re-epoch data\n",
    "            re_epoch_data.append(re_epoch)\n",
    "\n",
    "           # Append the stimulus type label (1 for Ch1, 2 for Ch2)\n",
    "            condition_labels.append(stim_type)\n",
    "\n",
    "# Convert the re-epoch data list to a NumPy array (n_epochs, n_channels, n_samples)\n",
    "re_epoch_data = np.array(re_epoch_data)\n",
    "print(re_epoch_data.shape)\n",
    "# Create a new MNE EpochsArray object with re-epoch data\n",
    "re_epochs = mne.EpochsArray(re_epoch_data, info)\n",
    "re_epochs = re_epochs.get_data()\n",
    "\n",
    "print(\"Condition labels:\", condition_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "34a29682-367a-44ae-929c-fa10e17f96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Trial Based Data\n",
    "\n",
    "# Parameters for plotting\n",
    "trial_id = 3  # Select the trial you want to plot\n",
    "channel_id = 3  # Select the specific channel you want to plot (e.g., 3rd channel)\n",
    "tmin, tmax = -0.2, 0.8  # Time window used in re-epoching\n",
    "sfreq = 250  # Sampling frequency\n",
    "\n",
    "# Extract the single trial and single channel data\n",
    "single_trial = filtered_data[trial_id, channel_id, :]  # Extract a specific trial and channel\n",
    "\n",
    "# Generate the time vector corresponding to tmin and tmax\n",
    "n_samples = single_trial.shape[0]\n",
    "time = np.linspace(tmin, tmax, n_samples)  # Time array for x-axis (in seconds)\n",
    "\n",
    "# Baseline correction: Subtract the mean of the pre-stimulus period (e.g., tmin to 0)\n",
    "baseline_start = int((tmin - tmin) * sfreq)  # Index of the start of the baseline (relative to tmin)\n",
    "baseline_end = int((0 - tmin) * sfreq)  # Index of the end of the baseline (0 seconds)\n",
    "baseline_mean = np.mean(single_trial[baseline_start:baseline_end])  # Calculate the baseline mean\n",
    "\n",
    "# Apply baseline correction\n",
    "single_trial_baseline_corrected = single_trial - baseline_mean\n",
    "\n",
    "# Plot the single channel data (with baseline correction)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, single_trial_baseline_corrected, label=ch_names[channel_id], color='b')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('EEG Amplitude (µV)')\n",
    "plt.title(f'Trial {trial_id + 1} - Channel {ch_names[channel_id]} (Baseline Corrected)')\n",
    "\n",
    "# Add a grid for better visualization\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9b7ae0d9-ca8a-4bce-bd0a-292359b8bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged data shape for label 1: (144, 12, 250)\n",
      "Averaged data shape for label 2: (144, 12, 250)\n"
     ]
    }
   ],
   "source": [
    "#averaging the every 10 trials based on stim channel 1 & 2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store the averaged data\n",
    "avg_label_1_per_group = []\n",
    "avg_label_2_per_group = []\n",
    "\n",
    "# Number of trials per group\n",
    "group_size = 10\n",
    "\n",
    "# Loop over the data in chunks of 10 trials\n",
    "for i in range(0, len(re_epoch_data), group_size):\n",
    "    # Extract the current group of 10 trials\n",
    "    group_data = re_epoch_data[i:i + group_size]\n",
    "    group_labels = condition_labels[i:i + group_size]\n",
    "    \n",
    "    # Check if we have exactly 10 trials in this group\n",
    "    if len(group_data) < group_size:\n",
    "        continue  # Skip if we have fewer than 10 trials at the end\n",
    "    \n",
    "    # Separate trials by label within the group\n",
    "    trials_label_1 = [group_data[j] for j in range(group_size) if group_labels[j] == 1]\n",
    "    trials_label_2 = [group_data[j] for j in range(group_size) if group_labels[j] == 2]\n",
    "    \n",
    "    # Ensure we have 5 trials of each label before averaging\n",
    "    if len(trials_label_1) == 5 and len(trials_label_2) == 5:\n",
    "        # Compute the averages for label \"1\" and label \"2\" trials within this group\n",
    "        avg_label_1 = np.mean(trials_label_1, axis=0)\n",
    "        avg_label_2 = np.mean(trials_label_2, axis=0)\n",
    "        \n",
    "        # Append to lists\n",
    "        avg_label_1_per_group.append(avg_label_1)\n",
    "        avg_label_2_per_group.append(avg_label_2)\n",
    "    else:\n",
    "        print(f\"Warning: Skipped group {i//group_size + 1} due to imbalance in labels\")\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "avg_label_1_per_group = np.array(avg_label_1_per_group)  # Shape (144, n_channels, n_samples)\n",
    "avg_label_2_per_group = np.array(avg_label_2_per_group)  # Shape (144, n_channels, n_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Averaged data shape for label 1:\", avg_label_1_per_group.shape)\n",
    "print(\"Averaged data shape for label 2:\", avg_label_2_per_group.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4168a8a0-d665-4048-a9aa-78585100a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data shape (label 1): (144, 12, 250)\n",
      "Normalized data shape (label 2): (144, 12, 250)\n"
     ]
    }
   ],
   "source": [
    "# Z-Score Normalization\n",
    "def z_score_normalize(data):\n",
    "    # data shape: (n_trials, n_channels, n_samples)\n",
    "    mean = np.mean(data, axis=(1, 2), keepdims=True)  # Compute mean for each trial\n",
    "    std = np.std(data, axis=(1, 2), keepdims=True)    # Compute std for each trial\n",
    "    normalized_data = (data - mean) / std            # Z-score normalization\n",
    "    return normalized_data\n",
    "\n",
    "# Normalize your datasets\n",
    "avg_label_1_normalized = z_score_normalize(avg_label_1_per_group)\n",
    "avg_label_2_normalized = z_score_normalize(avg_label_2_per_group)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Normalized data shape (label 1):\", avg_label_1_normalized.shape)\n",
    "print(\"Normalized data shape (label 2):\", avg_label_2_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9a4a5e92-85b6-4f91-bba5-c2895ad4d395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (288, 12, 250)\n",
      "Shape of y: (288,)\n",
      "Classification Accuracy with CSP: 0.72\n"
     ]
    }
   ],
   "source": [
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Combine data and labels for CSP\n",
    "X = np.concatenate([avg_label_1_per_group, avg_label_2_per_group], axis=0)  # Shape: (288, n_channels, n_samples)\n",
    "y = np.concatenate([np.ones(len(avg_label_1_per_group)), np.zeros(len(avg_label_2_per_group))])  # Labels\n",
    "\n",
    "# Reshape data if needed (already correct format)\n",
    "print(\"Shape of X:\", X.shape)  # Should be (n_trials, n_channels, n_samples)\n",
    "print(\"Shape of y:\", y.shape)  # Should be (n_trials,)\n",
    "\n",
    "# Initialize CSP\n",
    "csp = CSP(n_components=12, reg=None, log=True, cov_est='concat')  # 4 components for simplicity\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Fit CSP on training data\n",
    "csp.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data using CSP\n",
    "X_train_csp = csp.transform(X_train)  # CSP-transformed features for training\n",
    "X_test_csp = csp.transform(X_test)    # CSP-transformed features for testing\n",
    "\n",
    "# Train an SVM classifier on CSP-transformed features\n",
    "clf = SVC(kernel='linear', random_state=42)\n",
    "clf.fit(X_train_csp, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = clf.predict(X_test_csp)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy with CSP: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "27662159-bdfd-4543-9f2d-cc751a1c7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the averaged data sequence and channel wise\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "sequence_id = 55  # Choose a specific sequence (e.g., the first sequence out of 144)\n",
    "channel_id = 8  # Choose a specific channel to plot\n",
    "tmin, tmax = -0.2, 0.8  # Time window\n",
    "sfreq = 250  # Sampling frequency\n",
    "time = np.linspace(tmin, tmax, avg_label_1_per_group.shape[2])  # Time vector for x-axis\n",
    "\n",
    "# Plot for the chosen sequence and channel\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for label 1 (average of 5 trials with label 1 in the selected sequence)\n",
    "plt.plot(time, avg_label_1_normalized[sequence_id, channel_id, :], label='Label 1 Avg', color='b')\n",
    "\n",
    "# Plot for label 2 (average of 5 trials with label 2 in the selected sequence)\n",
    "plt.plot(time, avg_label_2_normalized[sequence_id, channel_id, :], label='Label 2 Avg', color='r')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('EEG Amplitude (µV)')\n",
    "plt.title(f'Averaged EEG Signal - Sequence {sequence_id + 1} - Channel {channel_id}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "219c3eb4-b1ab-4f5e-af08-a21ba481877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold accuracy: 98.28%\n",
      "Fold accuracy: 94.83%\n",
      "Fold accuracy: 94.83%\n",
      "Fold accuracy: 96.49%\n",
      "Fold accuracy: 96.49%\n",
      "Cross-Validation Accuracy:, 96.18%\n",
      "(288, 12, 250)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume avg_label_1_per_group and avg_label_2_per_group are (144, n_channels, n_samples)\n",
    "# Prepare data and labels\n",
    "data = np.concatenate([avg_label_1_per_group, avg_label_2_per_group], axis=0)  # (288, n_channels, n_samples)\n",
    "labels = np.array([1] * 144 + [2] * 144)  # Labels for each sample (1 for avg_label_1, 2 for avg_label_2)\n",
    "\n",
    "# Flatten each sequence for simplicity (you can also extract features as needed)\n",
    "data_flat = data.reshape(data.shape[0], -1)  # Reshape to (288, n_channels * n_samples)\n",
    "\n",
    "# Set up cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(data_flat):\n",
    "    X_train, X_test = data_flat[train_index], data_flat[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    clf = SVC(kernel='linear')  # Choose kernel type as appropriate\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Fold accuracy: {accuracy * 100:.2f}%')\n",
    "# Print average cross-validation accuracy\n",
    "print(f'Cross-Validation Accuracy:, {np.mean(accuracies) * 100:.2f}%')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "f531e079-4c03-44e7-80a5-8bd4f3cf3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-epoch data shape: (1440, 12, 150)\n",
      "Target side labels shape: (1440,)\n",
      "Label counts (1 for left, -1 for right): (array([-1,  1]), array([720, 720], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Parameters for re-epoching based on target side 1 & -1\n",
    "tmin, tmax = 0.2, 0.8  # Time window around the stimulus \n",
    "sfreq = 250  # Sampling frequency\n",
    "n_trials = filtered_data.shape[0]  # Total number of trials\n",
    "target_side_labels = []  # To store labels for each re-epoch based on target side (1 for left, -1 for right)\n",
    "\n",
    "# Create an empty list to hold re-epoch data\n",
    "re_epoch_data = []\n",
    "\n",
    "# Loop through each trial\n",
    "for trial_id in range(n_trials):\n",
    "    EEG_trial = filtered_data[trial_id, :, :]  # Extract the EEG data for this trial\n",
    "\n",
    "    # Get the stimulus onsets and target side for this trial\n",
    "    stim_ch1 = stim_id[0, :, trial_id]  # Stimulus channel 1\n",
    "    stim_ch2 = stim_id[1, :, trial_id]  # Stimulus channel 2\n",
    "    targetside_trial = Target[0, :, trial_id]  # Target side for this trial\n",
    "\n",
    "    # Find the indices (sample points) where stimulus onset occurs\n",
    "    stim_onsets_ch1 = np.where(stim_ch1 == 1)[0]  # Stimulus onset for Ch1\n",
    "    stim_onsets_ch2 = np.where(stim_ch2 == 1)[0]  # Stimulus onset for Ch2\n",
    "\n",
    "    # Combine stimulus onsets and create a list of (onset, stim_type)\n",
    "    all_stimuli_onsets = [(onset, 1) for onset in stim_onsets_ch1] + \\\n",
    "                         [(onset, 2) for onset in stim_onsets_ch2]\n",
    "\n",
    "    # Sort all stimuli onsets by time (ascending order)\n",
    "    all_stimuli_onsets.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Loop through each stimulus onset to re-epoch the data\n",
    "    for onset, stim_type in all_stimuli_onsets:\n",
    "        # Convert onset to seconds\n",
    "        onset_sec = onset / sfreq\n",
    "\n",
    "        # Find the sample indices for the time window around the stimulus onset\n",
    "        start_sample = int(onset + tmin * sfreq)\n",
    "        end_sample = int(onset + tmax * sfreq)\n",
    "\n",
    "        # Ensure indices are within bounds\n",
    "        if start_sample >= 0 and end_sample < EEG_trial.shape[1]:\n",
    "            # Extract the EEG segment for this stimulus onset\n",
    "            re_epoch = EEG_trial[:, start_sample:end_sample]\n",
    "\n",
    "            # Append to re-epoch data\n",
    "            re_epoch_data.append(re_epoch)\n",
    "\n",
    "            # Determine the target side at this stimulus onset\n",
    "            targetside_value = targetside_trial[onset]\n",
    "\n",
    "            # Assign labels directly based on target side\n",
    "            if targetside_value == 1:  # Target is on the left\n",
    "                target_side_labels.append(1)\n",
    "            elif targetside_value == -1:  # Target is on the right\n",
    "                target_side_labels.append(-1)\n",
    "\n",
    "# Convert the re-epoch data list to a NumPy array (n_epochs, n_channels, n_samples)\n",
    "re_epoch_data2 = np.array(re_epoch_data)\n",
    "print(\"Re-epoch data shape:\", re_epoch_data2.shape)  # Should be (1440, n_channels, n_samples)\n",
    "\n",
    "# Convert target side labels to a NumPy array\n",
    "target_side_labels = np.array(target_side_labels)\n",
    "print(\"Target side labels shape:\", target_side_labels.shape)  # Should be (1440,)\n",
    "\n",
    "# Verify the distribution of labels\n",
    "print(\"Label counts (1 for left, -1 for right):\", np.unique(target_side_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "2b9c35d5-a03b-4d05-807e-c18876ae8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged data shape for label 1: (144, 12, 150)\n",
      "Averaged data shape for label 2: (144, 12, 150)\n"
     ]
    }
   ],
   "source": [
    "#averaging the every 10 trials based on target side 1 & -1\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store the averaged data\n",
    "avg_label_P1_per_group = []\n",
    "avg_label_M1_per_group = []\n",
    "\n",
    "# Number of trials per group\n",
    "group_size = 10\n",
    "\n",
    "# Loop over the data in chunks of 10 trials\n",
    "for i in range(0, len(re_epoch_data2), group_size):\n",
    "    # Extract the current group of 10 trials\n",
    "    group_data = re_epoch_data2[i:i + group_size]\n",
    "    group_labels = target_side_labels[i:i + group_size]\n",
    "    \n",
    "    # Check if we have exactly 10 trials in this group\n",
    "    if len(group_data) < group_size:\n",
    "        continue  # Skip if we have fewer than 10 trials at the end\n",
    "    \n",
    "    # Separate trials by label within the group\n",
    "    trials_label_P1 = [group_data[j] for j in range(group_size) if group_labels[j] == 1]\n",
    "    trials_label_M1 = [group_data[j] for j in range(group_size) if group_labels[j] == -1]\n",
    "    \n",
    "    # Ensure we have 5 trials of each label before averaging\n",
    "    if len(trials_label_P1) == 5 and len(trials_label_M1) == 5:\n",
    "        # Compute the averages for label \"1\" and label \"2\" trials within this group\n",
    "        avg_label_P1 = np.mean(trials_label_P1, axis=0)\n",
    "        avg_label_M1 = np.mean(trials_label_M1, axis=0)\n",
    "        \n",
    "        # Append to lists\n",
    "        avg_label_P1_per_group.append(avg_label_P1)\n",
    "        avg_label_M1_per_group.append(avg_label_M1)\n",
    "    else:\n",
    "        print(f\"Warning: Skipped group {i//group_size + 1} due to imbalance in labels\")\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "avg_label_P1_per_group = np.array(avg_label_P1_per_group)  # Shape (144, n_channels, n_samples)\n",
    "avg_label_M1_per_group = np.array(avg_label_M1_per_group)  # Shape (144, n_channels, n_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Averaged data shape for label 1:\", avg_label_P1_per_group.shape)\n",
    "print(\"Averaged data shape for label 2:\", avg_label_M1_per_group.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "2fbfcf86-0e8e-43d4-8cba-2054ea52591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (288, 12, 150)\n",
      "Shape of y: (288,)\n",
      "Classification Accuracy with CSP: 0.71\n"
     ]
    }
   ],
   "source": [
    "#classification of target side based epochs by CSP\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Combine data and labels for CSP\n",
    "X = np.concatenate([avg_label_P1_per_group, avg_label_M1_per_group], axis=0)  # Shape: (288, n_channels, n_samples)\n",
    "y = np.concatenate([np.ones(len(avg_label_P1_per_group)), np.zeros(len(avg_label_M1_per_group))])  # Labels\n",
    "\n",
    "# Reshape data if needed (already correct format)\n",
    "print(\"Shape of X:\", X.shape)  # Should be (n_trials, n_channels, n_samples)\n",
    "print(\"Shape of y:\", y.shape)  # Should be (n_trials,)\n",
    "\n",
    "# Initialize CSP\n",
    "csp = CSP(n_components=4, reg=None, log=True, cov_est='concat')  # 4 components for simplicity\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Fit CSP on training data\n",
    "csp.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data using CSP\n",
    "X_train_csp = csp.transform(X_train)  # CSP-transformed features for training\n",
    "X_test_csp = csp.transform(X_test)    # CSP-transformed features for testing\n",
    "\n",
    "# Train an SVM classifier on CSP-transformed features\n",
    "clf = SVC(kernel='linear', random_state=42)\n",
    "clf.fit(X_train_csp, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = clf.predict(X_test_csp)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy with CSP: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "7f247a35-85af-41c4-b30e-eeedb1ae14a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical Correlation: 0.025905066377166313\n",
      "Spatial Filter (a): [[-0.20452624]\n",
      " [ 0.02462186]\n",
      " [-0.03272652]\n",
      " [-0.05137434]\n",
      " [ 0.2187501 ]\n",
      " [-0.26258401]\n",
      " [ 0.5134056 ]\n",
      " [ 0.10543234]\n",
      " [-0.43299972]\n",
      " [-0.07916486]\n",
      " [-0.30657381]\n",
      " [ 0.52402847]]\n",
      "Target Weights (b): [[-1.]]\n",
      "Classification Accuracy: 0.5059166666666667\n"
     ]
    }
   ],
   "source": [
    "#CCA_transformation of data with target side 1 & -1\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "# Step 1: Prepare EEG Data (X)\n",
    "# Flatten EEG data (X) across epochs and time points\n",
    "n_epochs, n_channels, n_samples = re_epoch_data2.shape\n",
    "X = re_epoch_data2.transpose(0, 2, 1).reshape(n_epochs * n_samples, n_channels)  # Shape: (1440 * 225, 12)\n",
    "\n",
    "# Step 2: Construct Target Matrix (Y)\n",
    "# Create identity matrices for left (1) and right (-1) targets\n",
    "Y = np.zeros((n_epochs * n_samples, 1))\n",
    "for i, label in enumerate(target_side_labels):  # target_side_labels has shape (1440,)\n",
    "    start_idx = i * n_samples\n",
    "    end_idx = (i + 1) * n_samples\n",
    "    if label == 1:  # Left target\n",
    "        Y[start_idx:end_idx, 0] = 1 * np.eye(n_samples)[:, 0]  # Positive identity for left\n",
    "    elif label == -1:  # Right target\n",
    "        Y[start_idx:end_idx, 0] = -1 * np.eye(n_samples)[:, 0]  # Negative identity for right\n",
    "\n",
    "# Step 3: Apply Canonical Correlation Analysis (CCA)\n",
    "cca = CCA(n_components=1)  # One canonical component\n",
    "cca.fit(X, Y)\n",
    "\n",
    "# Step 4: Transform Data Using CCA\n",
    "X_c, Y_c = cca.transform(X, Y)\n",
    "\n",
    "# Step 5: Compute Canonical Correlation\n",
    "correlation = np.corrcoef(X_c[:, 0], Y_c[:, 0])[0, 1]\n",
    "print(\"Canonical Correlation:\", correlation)\n",
    "\n",
    "# Step 6: Extract Weight Vectors\n",
    "a = cca.x_weights_  # Spatial filter (weights for EEG data)\n",
    "b = cca.y_weights_  # Weights for target matrix\n",
    "print(\"Spatial Filter (a):\", a)\n",
    "print(\"Target Weights (b):\", b)\n",
    "\n",
    "# Step 7: Canonical Components\n",
    "u = X @ a  # EEG-derived time series\n",
    "v = Y @ b  # Target-derived time series\n",
    "\n",
    "# Classification Based on Sign of Canonical Correlation\n",
    "predictions = np.sign(u.flatten())  # Predict left (1) or right (-1)\n",
    "true_labels = np.repeat(target_side_labels, n_samples)  # Expand target labels\n",
    "accuracy = np.mean(predictions == true_labels)\n",
    "print(\"Classification Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "735d37af-bf94-4339-ad8a-3c83fba6c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold accuracy: 74.14%\n",
      "Fold accuracy: 79.31%\n",
      "Fold accuracy: 81.03%\n",
      "Fold accuracy: 87.72%\n",
      "Fold accuracy: 82.46%\n",
      "Cross-Validation Accuracy:, 80.93%\n",
      "(288, 12, 150)\n"
     ]
    }
   ],
   "source": [
    "#k-fold CV with SVM for labels with target side 1 & -1\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume avg_label_1_per_group and avg_label_2_per_group are (144, n_channels, n_samples)\n",
    "# Prepare data and labels\n",
    "data = np.concatenate([avg_label_P1_per_group, avg_label_M1_per_group], axis=0)  # (288, n_channels, n_samples)\n",
    "labels = np.array([1] * 144 + [-1] * 144)  # Labels for each sample (1 for avg_label_P1, -1 for avg_label_M1)\n",
    "\n",
    "# Flatten each sequence for simplicity (you can also extract features as needed)\n",
    "data_flat = data.reshape(data.shape[0], -1)  # Reshape to (288, n_channels * n_samples)\n",
    "\n",
    "# Set up cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(data_flat):\n",
    "    X_train, X_test = data_flat[train_index], data_flat[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    clf = SVC(kernel='linear')  # Choose kernel type as appropriate\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Fold accuracy: {accuracy * 100:.2f}%')\n",
    "# Print average cross-validation accuracy\n",
    "print(f'Cross-Validation Accuracy:, {np.mean(accuracies) * 100:.2f}%')\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
